# RAG 활용 지리과 서답형 문항 유형별 자동채점 플랫폼

## 1. 플랫폼 소개

본 플랫폼은 RAG(Retrieval-Augmented Generation) 기술을 활용하여 지리과 서답형 문항에 대한 학생 답안을 자동으로 채점하고 피드백을 제공하는 웹 기반 도구입니다. 교사가 문항 개발에 사용된 원본 자료와 평가 루브릭, 학생 답안을 업로드하면, LLM(Large Language Model)이 관련성 높은 정보를 검색하고 루브릭에 따라 채점 및 상세 피드백을 생성합니다. 채점 결과는 시각화된 대시보드를 통해 한눈에 파악할 수 있으며, Excel 파일로 다운로드하여 관리할 수 있습니다.

## 2. 주요 기능

### 2.1. 초기 환경 설정 및 프로젝트 구조
-   **가상 환경 및 라이브러리 관리**: Python 가상 환경 설정 및 `requirements.txt`를 통한 필수 라이브러리 관리를 지원합니다.
-   **.env 파일 설정**: API 키 등 민감한 환경 변수를 안전하게 관리합니다.

### 2.2. 데이터 준비 및 벡터 DB 구축
-   **Source Data 로더**: PDF, Excel, Word, Text 등 다양한 형식의 원본 자료를 업로드하고 로드할 수 있습니다.
-   **Chunking 및 Embedding**: 로드된 문서를 효율적인 처리를 위해 청크로 분할하고, Hugging Face `nlpai-lab/KURE-v1` 모델을 사용하여 임베딩합니다.
-   **FAISS Vector DB**: 임베딩된 청크를 기반으로 FAISS 벡터 데이터베이스를 구축하고 저장/로드할 수 있습니다.

### 2.3. LLM 모델 선택 및 API 관리
-   **LLM 모델 선택 UI**: Streamlit을 통해 GROQ, OpenAI, Google Gemini 등 다양한 LLM 제공사와 세부 모델을 선택할 수 있습니다.
-   **LLM API 호출 래퍼**: 여러 개의 API 키를 순환하며 사용하고, API 호출 실패 시 재시도 로직을 통해 안정적인 LLM 연동을 지원합니다.

### 2.4. 평가 기준 및 학생 답안 입력
-   **문항 유형 선택**: 단답형, 제한형, 확장형 등 문항 유형을 선택할 수 있습니다.
-   **평가 루브릭 입력**: 주요 채점 요소와 세부 채점 요소를 동적으로 추가/삭제하며 상세한 평가 루브릭을 직접 구성할 수 있습니다.
-   **학생 답안 업로드**: Excel 파일로 학생 답안을 일괄 업로드하고, 필요한 컬럼(이름, 학년, 반, 번호, 답안)을 자동으로 파싱합니다.

### 2.5. RAG 기반 채점 로직 및 피드백 생성
-   **동적 프롬프트 템플릿**: 선택된 문항 유형과 평가 루브릭, 학생 답안, 검색된 문서를 바탕으로 LLM에 최적화된 프롬프트를 동적으로 생성합니다.
-   **유사 문서 검색 (Retrieval)**: 학생 답안과 관련된 원본 자료의 유사 문서를 FAISS 벡터 DB에서 효율적으로 검색합니다.
-   **Rerank 로직**: 검색된 문서들을 `Dongjin-kr/ko-reranker` 모델을 사용하여 쿼리(학생 답안)와의 관련성 기준으로 재정렬하여 LLM에 더 정확한 컨텍스트를 제공합니다.
-   **LLM 기반 채점 및 피드백**: LLM이 검색된 문서와 루브릭을 활용하여 학생 답안을 채점하고, 교과 내용 피드백 및 의사 응답 여부 판단을 포함한 상세 피드백을 생성합니다. 병렬 처리를 통해 다수의 학생 답안을 빠르게 처리합니다.

### 2.6. 최종 결과 출력 및 시각화
-   **채점 결과 표시**: 루브릭별 점수, 최종 합산 점수, 상세 피드백, 참고 문서 정보 등을 직관적으로 표시합니다.
-   **대시보드 시각화**: 반 전체 평균 점수, 점수 분포, 루브릭 항목별 평균 점수 등 통계 데이터를 시각적으로 제공하며, 개별 학생의 상세 분석도 가능합니다.
-   **Excel 다운로드**: 채점된 모든 결과를 Excel 파일로 다운로드하여 편리하게 관리할 수 있습니다.

## 3. 설치 및 실행 방법

### 3.1. 가상 환경 설정 (uv 사용 권장)

```bash
# uv 설치 (설치되어 있지 않다면)
pip install uv

# 가상 환경 생성
uv venv

# 가상 환경 활성화
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
```

### 3.2. 필수 라이브러리 설치

가상 환경을 활성화한 후, 프로젝트 루트에 있는 `requirements.txt` 파일을 사용하여 필요한 라이브러리를 설치합니다.

```bash
uv pip install -r requirements.txt
```

### 3.3. `.env` 파일 설정

프로젝트 루트에 `.env` 파일을 생성하고, 사용할 LLM 제공사의 API 키를 설정합니다. 여러 개의 키를 사용할 경우 `_1`, `_2` 등을 붙여 추가할 수 있습니다.

예시:

```
OPENAI_API_KEY=sk-your_openai_api_key
GOOGLE_API_KEY=your_google_api_key
GROQ_API_KEY=your_groq_api_key

# 여러 개의 키 사용 예시
OPENAI_API_KEY_1=sk-your_openai_api_key_1
OPENAI_API_KEY_2=sk-your_openai_api_key_2
```

### 3.4. 애플리케이션 실행

모든 설정이 완료되면, 다음 명령어를 사용하여 Streamlit 애플리케이션을 실행합니다.

```bash
streamlit run main.py
```

명령어 실행 후 웹 브라우저에 애플리케이션이 자동으로 열립니다. (일반적으로 `http://localhost:8501`)

## 4. 향후 개선 사항

-   **단위 및 통합 테스트 작성**: 코드의 안정성과 유지보수성을 위해 각 모듈별 단위 테스트 및 전체 파이프라인의 통합 테스트를 작성하는 것이 필요합니다.

## 5. 프로젝트 배포

본 프로젝트는 GitHub에 푸시하고 Streamlit Cloud를 통해 웹 애플리케이션으로 배포할 수 있습니다. 상세한 절차는 `push.md` 파일을 참고해주세요.

### 5.1. GitHub에 푸시

1.  `.gitignore` 파일에 `.env`, `.venv/` 등 민감 정보 및 불필요한 파일/폴더가 포함되어 있는지 확인합니다.
2.  Git 로컬 리포지토리를 초기화하고 커밋합니다.
3.  GitHub에서 새 리포지토리를 생성하고, 로컬 리포지토리와 연결하여 코드를 푸시합니다.

### 5.2. Streamlit Cloud 배포

1.  `pip freeze > requirements.txt` 명령어를 사용하여 `requirements.txt` 파일을 최신 상태로 업데이트합니다.
2.  Streamlit Cloud에 로그인하여 새 앱을 생성합니다.
3.  GitHub 리포지토리와 `main.py` (또는 메인 앱 파일) 경로를 지정합니다.
4.  **가장 중요**: `Advanced settings...`의 **Secrets** 섹션에 `.env` 파일의 내용을 복사하여 붙여넣어 민감 정보를 설정합니다.
5.  `Deploy!` 버튼을 클릭하여 배포를 시작합니다.